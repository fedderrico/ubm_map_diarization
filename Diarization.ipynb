{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBM MAP Adaptation approach\n",
    "\n",
    "Process steps:\n",
    "1. Train Univeral Background Model (UBM) - a speaker independent distribution of acoustic features\n",
    "2. Apply VAD to make segments\n",
    "3. Using Maximum A-Posterior (MAP) adaptation build model for each segment\n",
    "4. Extract mean supervector from each model\n",
    "5. Using spectral clustering algo separate speakers\n",
    "\n",
    "\n",
    "<img src=\"img/gmm_map2.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import pickle\n",
    "import contextlib\n",
    "import librosa\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "import webrtcvad\n",
    "import collections\n",
    "import copy\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def extract_features(y, sr, window, hop, n_mfcc):\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=int(hop*sr), n_fft=int(window*sr), n_mfcc=n_mfcc, dct_type=2)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    stacked = np.vstack((mfcc, mfcc_delta, mfcc_delta2))\n",
    "    return stacked.T\n",
    "\n",
    "# code modified for compactness\n",
    "# orignal code https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "class Frame(object):\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = False\n",
    "                yield b''.join([f.bytes for f in voiced_frames])\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if voiced_frames:\n",
    "        yield b''.join([f.bytes for f in voiced_frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_adaptation(gmm, data, max_iterations = 300, likelihood_threshold = 1e-20, relevance_factor = 16):\n",
    "    N = data.shape[0]\n",
    "    D = data.shape[1]\n",
    "    K = gmm.n_components\n",
    "    \n",
    "    mu_new = np.zeros((K,D))\n",
    "    n_k = np.zeros((K,1))\n",
    "    \n",
    "    mu_k = gmm.means_\n",
    "    cov_k = gmm.covariances_\n",
    "    pi_k = gmm.weights_\n",
    "\n",
    "    old_likelihood = gmm.score(data)\n",
    "    new_likelihood = 0\n",
    "    iterations = 0\n",
    "    while(abs(old_likelihood - new_likelihood) > likelihood_threshold and iterations < max_iterations):\n",
    "        iterations += 1\n",
    "        old_likelihood = new_likelihood\n",
    "        z_n_k = gmm.predict_proba(data)\n",
    "        n_k = np.sum(z_n_k,axis = 0)\n",
    "\n",
    "        for i in range(K):\n",
    "            temp = np.zeros((1,D))\n",
    "            for n in range(N):\n",
    "                temp += z_n_k[n][i]*data[n,:]\n",
    "            mu_new[i] = (1/n_k[i])*temp\n",
    "\n",
    "        adaptation_coefficient = n_k/(n_k + relevance_factor)\n",
    "        for k in range(K):\n",
    "            mu_k[k] = (adaptation_coefficient[k] * mu_new[k]) + ((1 - adaptation_coefficient[k]) * mu_k[k])\n",
    "        gmm.means_ = mu_k\n",
    "\n",
    "        log_likelihood = gmm.score(data)\n",
    "        new_likelihood = log_likelihood\n",
    "        print(log_likelihood)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setings\n",
    "SR = 8000 # sample rate\n",
    "N_MFCC = 13 # number of MFCC to extract\n",
    "N_FFT = 0.032  # length of the FFT window in seconds\n",
    "HOP_LENGTH = 0.010 # number of samples between successive frames in seconds\n",
    "\n",
    "N_COMPONENTS = 16 # number of gaussians\n",
    "COVARINACE_TYPE = 'full' # cov type for GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "LOAD_SIGNAL = False\n",
    "if LOAD_SIGNAL:\n",
    "    y, sr = librosa.load('data/2018-08-26-beseda-1616.mp3', sr=SR)\n",
    "    pre_emphasis = 0.97\n",
    "    y = np.append(y[0], y[1:] - pre_emphasis * y[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKE_CHUNKS = False\n",
    "\n",
    "if MAKE_CHUNKS:\n",
    "    vad = webrtcvad.Vad(2)\n",
    "    audio = np.int16(y/np.max(np.abs(y)) * 32768)\n",
    "\n",
    "    frames = frame_generator(10, audio, sr)\n",
    "    frames = list(frames)\n",
    "    segments = vad_collector(sr, 50, 200, vad, frames)\n",
    "\n",
    "    if not os.path.exists('data/chunks'): os.makedirs('data/chunks')\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        chunk_name = 'data/chunks/chunk-%003d.wav' % (i,)\n",
    "        write_wave(chunk_name, segment[0: len(segment)-int(100*sr/1000)], sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract MFCC, first and second derivatives\n",
    "FEATURES_FROM_FILE = True\n",
    "\n",
    "feature_file_name = 'data/features_{0}.pkl'.format(N_MFCC)\n",
    "\n",
    "if FEATURES_FROM_FILE:\n",
    "    ubm_features=pickle.load(open(feature_file_name, 'rb'))\n",
    "else:\n",
    "    ubm_features = extract_features(np.array(y), sr, window=N_FFT, hop=HOP_LENGTH, n_mfcc=N_MFCC)\n",
    "    ubm_features = preprocessing.scale(ubm_features)\n",
    "    pickle.dump(ubm_features, open(feature_file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-44.41553354487879\n"
     ]
    }
   ],
   "source": [
    "# UBM Train\n",
    "UBM_FROM_FILE = True\n",
    "\n",
    "ubm_file_name = 'data/ubm_{0}_{1}_{2}MFCC.pkl'.format(N_COMPONENTS, COVARINACE_TYPE, N_MFCC)\n",
    "\n",
    "if UBM_FROM_FILE:\n",
    "    ubm=pickle.load(open(ubm_file_name, 'rb'))\n",
    "else:\n",
    "    ubm = GaussianMixture(n_components = N_COMPONENTS, covariance_type = COVARINACE_TYPE)\n",
    "    ubm.fit(ubm_features)\n",
    "    pickle.dump(ubm, open(ubm_file_name, \"wb\"))\n",
    "    \n",
    "print(ubm.score(ubm_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 624)\n"
     ]
    }
   ],
   "source": [
    "SV = []\n",
    "\n",
    "for i in range(101):\n",
    "    clear_output(wait=True)\n",
    "    fname='data/chunks/chunk-%003d.wav' % (i,)\n",
    "    print('UBM MAP adaptation for {0}'.format(fname))\n",
    "    y_, sr_ = librosa.load(fname, sr=None)\n",
    "    f_ = extract_features(y_, sr_, window=N_FFT, hop=HOP_LENGTH, n_mfcc=N_MFCC)\n",
    "    f_ = preprocessing.scale(f_)\n",
    "    gmm = copy.deepcopy(ubm)\n",
    "    gmm = map_adaptation(gmm, f_, max_iterations=1, relevance_factor=16)\n",
    "    sv = gmm.means_.flatten()\n",
    "    sv = preprocessing.scale(sv)\n",
    "    SV.append(sv)\n",
    "\n",
    "SV = np.array(SV)\n",
    "clear_output()\n",
    "print(SV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 2, 25, 26, 46, 48, 49, 61, 85, 86]\n"
     ]
    }
   ],
   "source": [
    "N_CLUSTERS = 2\n",
    "\n",
    "def rearrange(labels, n):\n",
    "    seen = set()\n",
    "    distinct = [x for x in labels if x not in seen and not seen.add(x)]\n",
    "    correct = [i for i in range(n)]\n",
    "    dict_ = dict(zip(distinct, correct))\n",
    "    return [x if x not in dict_ else dict_[x] for x in labels]\n",
    "\n",
    "sc = SpectralClustering(n_clusters=N_CLUSTERS, affinity='cosine')\n",
    "labels = sc.fit_predict(SV)\n",
    "labels = rearrange(labels, N_CLUSTERS)\n",
    "print(labels)\n",
    "# выведем номера сегментов, где говорит девушка.\n",
    "print([i for i, x in enumerate(labels) if x == 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
